---
layout: post
title: Milestone 2
---

## Table des matières

1. [Ingénierie des caractéristiques I](#ingenierie_des_caracteristiques_1)
2. [Suivi des expériences](#suivi_experiences)
3. [Modèles de base](#modeles_de_base)
4. [Ingénierie des caractéristiques II](#ingenierie_des_caracteristiques_2)
5. [Modèles avancés](#modeles_avances)
6. [Meilleur modèle (Faites de votre mieux!)](#meilleur_modele)
7. [Évaluer sur l'ensemble de test](#evaluer_ensemble_test)

## 1. Suivi des expériences

## 2. Ingénierie des caractéristiques I

## 3. Modèles de base

Notre premier modèle de base (entraîné seulement avec la distance du but) a une précision de 0.905. Or, quand nous regardons les prédictions de notre modèle, 
nous remarquons qu'il prédit toujours qu'un tir ne résultera pas en un but. Évidemment, comme la plupart des tirs ne résultent pas en un but, notre modèle obtient
une excellente précision. Toutefois, nous souhaitons être capable de prédire les buts. Pour la suite de l'expérience, il nous faudra donc procéder
à un rééquilibrage des données.

## 4. Ingénierie des caractéristiques II

## 5. Modèles avancés

#### 5.1 XGBoost avec les caractéristiques des base
Nous avons premièrement entraîné un modèle XGBoost en n'utilisant que les caractérisqitques de distance et d'angles...(TODO)

Nous avons ensuite entraîné un modèle XGBoost avec les caractéristiques créées dans la section 4. Nous avons créé une fonction se servant d'un hypercube latin afin de trouver le meilleur choix d'hyperparamètres. Voici les hyperparamètres sélectionnés:

<ul>
  <li>learning_rate: </li>
  <li>n_estimators: </li>
  <li>max_depth: </li>
  <li>min_child_weight: </li>
  <li>subsample: </li>
  <li>colsample_bytree: </li>
  <li>reg_alpha: </li>
  <li>max_delta_step: </li>
  <li>min_split_loss: </li>
</ul>

## 6. Meilleur modèle (Faites de votre mieux!)

## 7. Évaluer sur l'ensemble de test
